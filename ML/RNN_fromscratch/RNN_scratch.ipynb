{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from d2l import torch as d2l_torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size, num_steps = 32, 35\n",
    "train_iter, vocab = d2l_torch.load_data_time_machine(batch_size=batch_size, num_steps=num_steps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this simple RNN models, there are steps to follow:\n",
    "1. To build the model, we need:\n",
    "- A function to make the params for RNN model's layers, including OutputLayer and RNNLayer.\n",
    "- A function to init the state of the model, which is a tensor of size (batch_size, num_hiddens).\n",
    "- A forward function to tell the net how to work with the data.\n",
    "- A wrapping class to hold all params and function.\n",
    "2. To train and test the model, we need:\n",
    "- A train function to train model (ofcourse :P)\n",
    "- A predict function to see how well model do with our own eyes.\n",
    "- To measure performance, we use 'perplexity'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create layers params\n",
    "def get_params(vocab_size, num_hiddens, device):\n",
    "    num_inputs = num_outputs = vocab_size\n",
    "\n",
    "    def normal(shape):\n",
    "        return torch.randn(size=shape, device=device) * 0.01\n",
    "\n",
    "    # Hidden Layer Params\n",
    "    W_xh = normal((num_inputs, num_hiddens))\n",
    "    W_hh = normal((num_hiddens, num_hiddens))\n",
    "    b_h = torch.zeros(num_hiddens, device=device)\n",
    "    # Output Layer Params\n",
    "    W_hq = normal((num_hiddens, num_outputs))\n",
    "    b_q = torch.zeros(num_outputs, device=device)\n",
    "    # Attach gradient for params\n",
    "    params = [W_xh, W_hh, b_h, W_hq, b_q]\n",
    "    for param in params:\n",
    "        param.requires_grad_(True)\n",
    "    return params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize model state\n",
    "def init_state(batch_size, num_hiddens, device):\n",
    "    return (torch.zeros((batch_size, num_hiddens), device=device), )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rnn_forward(inputs, state, params):\n",
    "    # Unpack params\n",
    "    W_xh, W_hh, b_h, W_hq, b_q = params\n",
    "    outputs = []\n",
    "    H, = state\n",
    "    for X in inputs:\n",
    "        H = torch.tanh(torch.mm(X, W_xh) + torch.mm(H, W_hh) + b_h)\n",
    "        Y = torch.mm(H, W_hq) + b_q\n",
    "        outputs.append(Y)\n",
    "    return torch.cat(outputs, dim=0), (H, )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN:\n",
    "    def __init__(self, forward_fn, init_state, num_hiddens, vocab_size, device) -> None:\n",
    "        self.params = get_params(vocab_size, num_hiddens, device=device)\n",
    "        self.forward_fn = forward_fn\n",
    "        self.init_state = init_state\n",
    "        self.num_hiddens = num_hiddens\n",
    "        self.vocab_size = vocab_size\n",
    "    \n",
    "    def __call__(self, X, state):\n",
    "        X = F.one_hot(X.T, self.vocab_size).type(torch.float32)\n",
    "        return self.forward_fn(X, state, self.params)\n",
    "\n",
    "    def begin_state(self, batch_size, device):\n",
    "        return self.init_state(batch_size, self.num_hiddens, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grad_clipping(net, theta):\n",
    "    params = net.params\n",
    "    norm = torch.sqrt(sum(torch.sum((p.grad**2)) for p in params))\n",
    "    if norm > theta:\n",
    "        for param in params:\n",
    "            param.grad[:] *= theta / norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(net, train_iter, loss, updater, device):\n",
    "    state = None\n",
    "    metric = d2l_torch.Accumulator(2)\n",
    "    for X, Y in train_iter:\n",
    "        if state is None:\n",
    "            state = net.begin_state(batch_size=X.shape[0], device=device)\n",
    "        else:\n",
    "            for s in state:\n",
    "              s.detach_()\n",
    "        y = Y.T.reshape(-1)\n",
    "        X, y = X.to(device), y.to(device)\n",
    "        Y_hat, state = net(X, state)\n",
    "        l = loss(Y_hat, y.long()).mean()\n",
    "        l.backward()\n",
    "        grad_clipping(net, 1)\n",
    "        updater(batch_size=1)\n",
    "        metric.add(l*y.numel(), y.numel())\n",
    "    return math.exp(metric[0]/metric[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_rnn(net, train_iter, num_epochs, learning_rate):\n",
    "    loss = nn.CrossEntropyLoss()\n",
    "    updater = lambda batch_size: d2l_torch.sgd(net.params, learning_rate, batch_size)\n",
    "    best_perplexity = 99999\n",
    "    patience = 5 # early stopping to prevent overshooting to overfitting zone\n",
    "    for epoch in range(num_epochs):\n",
    "        perplexity = train_epoch(net, train_iter, loss, updater, d2l_torch.try_gpu())\n",
    "        if perplexity < best_perplexity:\n",
    "            best_perplexity = perplexity\n",
    "            patience = 5 # reset patience if manage to reduce perplexity\n",
    "        else:\n",
    "            if patience < 0:\n",
    "                break\n",
    "            patience -= 1\n",
    "        if(epoch % 50 == 0):\n",
    "            print(f\"Epoch: {epoch:d}/{num_epochs:d}| Perplexity: {perplexity:.2f}\")\n",
    "    print(f\"Perplexity: {perplexity:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(net, prefix, num_preds, device):\n",
    "    state = net.begin_state(batch_size=1, device=device)\n",
    "    outputs = [vocab[prefix[0]]]\n",
    "    get_input = lambda: torch.tensor([outputs[-1]], device=device).reshape((1, 1))\n",
    "    for y in prefix[1:]:  # Warm-up period\n",
    "        _, state = net(get_input(), state)\n",
    "        outputs.append(vocab[y])\n",
    "    for _ in range(num_preds):  # Predict `num_preds` steps\n",
    "        y, state = net(get_input(), state)\n",
    "        outputs.append(int(y.argmax(dim=1).reshape(1)))\n",
    "    return ''.join([vocab.idx_to_token[i] for i in outputs])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs, lr = 1500, 0.1\n",
    "num_hiddens = 512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = RNN(forward_fn=rnn_forward, \n",
    "            init_state=init_state, \n",
    "            vocab_size=len(vocab), \n",
    "            num_hiddens=num_hiddens, \n",
    "            device=d2l_torch.try_gpu())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0/1500| Perplexity: 27.62\n",
      "Epoch: 50/1500| Perplexity: 16.57\n",
      "Epoch: 100/1500| Perplexity: 13.01\n",
      "Epoch: 150/1500| Perplexity: 11.02\n",
      "Epoch: 200/1500| Perplexity: 10.24\n",
      "Epoch: 250/1500| Perplexity: 9.66\n",
      "Epoch: 300/1500| Perplexity: 9.27\n",
      "Perplexity: 9.17\n"
     ]
    }
   ],
   "source": [
    "train_rnn(net, train_iter=train_iter, num_epochs=num_epochs, learning_rate=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'time traveller and the the the the '"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict(net, \"time traveller \", 20, d2l_torch.try_gpu())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After altering hyperparams, the result is:\n",
    "- Scaling layer params help stablelizing the training process.\n",
    "    - Extreme scaling (too big/too small of the multiplier) can be compensated by adjusting learning rate, but will still resulted in unstable training.\n",
    "    - For this example, scaled by 0.01 displays best perplexity."
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "ec7e58b3a9bb02cd090814cef287ec8aadd79361ec07b9179adbce85ffc378b8"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit ('pytorch': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
